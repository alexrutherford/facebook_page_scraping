{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests,re\n",
    "import sys\n",
    "sys.path.append('/usr/local/lib/python2.7/dist-packages/')\n",
    "import textblob\n",
    "import langid\n",
    "import logging\n",
    "import collections\n",
    "import pymongo,time\n",
    "from secrets import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "Script to scrape content from Facebook API.\n",
    "Collects\n",
    "1. Pages matching a keyword query\n",
    "2. Recent posts from these pages (limit set by API)\n",
    "3. Comments on these posts (it seems that likes on comments are not available)\n",
    "4. Likes on these posts  \n",
    "\n",
    "### Errors\n",
    "\n",
    "Needs robust error handling due to errors with API requests. More detail [here](https://developers.facebook.com/docs/graph-api/using-graph-api/v2.4#errors)\n",
    "\n",
    "* ``Error 500 {\"error\":{\"message\":\"An unexpected error has occurred. Please retry your request later.\",\"type\":\"OAuthException\",\"is_transient\":true,\"code\":2}}``\n",
    "* ``Error 500 {\"error\":{\"code\":1,\"message\":\"An unknown error occurred\"}}``\n",
    "* ``ConnectionError: ('Connection aborted.', gaierror(-2, 'Name or service not known'))``\n",
    "\n",
    "### TODO\n",
    "\n",
    "* ~~Get likes on comments~~\n",
    "* ~~Have a restart page ID, so we can pick up if scraping fails~~\n",
    "* Write a script to check for new posts on pages that already exist in DB \n",
    "* Add a record of when data was pulled  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mongo tutorial [here](http://api.mongodb.org/python/current/tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up query to grab pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "ands=[]\n",
    "#ands.append('أوروبا') # Europe\n",
    "#ands.append('ألمانيا')\n",
    "#ands.append('المانيا') # Germany (two alternate spellings)\n",
    "\n",
    "ors=[]\n",
    "ors.append('مهاجرون') # Refugees\n",
    "ors.append('مهاجرين') # Refugees\n",
    "ors.append('المهاجرين') # The refugees\n",
    "ors.append('المهاجرون') # The refugees\n",
    "ors.append('المهاجرون') # The refugees\n",
    "ors.append('مهاجرون') # refugees\n",
    "ors.append('هجرة') # Migration\n",
    "ors.append('الهجرة') # The migration\n",
    "ors.append('أوروبا') # Europe\n",
    "ors.append('سوريا') # Syria\n",
    "\n",
    "#ors=[]\n",
    "ors.append('مهرب') # Trafficker\n",
    "ors.append('المتاجرين') # Traffickers\n",
    "\n",
    "#QUERY='+'.join(ands)+'+'\n",
    "QUERY='|'.join(ors)\n",
    "#QUERY='syria'\n",
    "#QUERY='سوريا'\n",
    "#QUERY='أوروبا' # Europe\n",
    "#QUERY=u'اوروبا'\n",
    "#QUERY=u'اللجوء' # asylum\n",
    "#QUERY=u'ملجأ' # asylum\n",
    "# 'Europe' + 'ANY ['migration']...'\n",
    "LIMIT=1000\n",
    "# Page limit\n",
    "postsLimit=250\n",
    "# Hard limit from API=250\n",
    "\n",
    "nSkip=5\n",
    "# Hit API for query nSkip times\n",
    "# before skipping\n",
    "\n",
    "nWait=60\n",
    "# Wait between API errors\n",
    "\n",
    "postSleepTime=0.5\n",
    "pageSleepTime=10\n",
    "# Pause so API not thrashed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "مهاجرون|مهاجرين|المهاجرين|المهاجرون|المهاجرون|مهاجرون|هجرة|الهجرة|أوروبا|سوريا|مهرب|المتاجرين\n"
     ]
    }
   ],
   "source": [
    "print QUERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1151 pages\n",
      "102108 posts\n",
      "208796 comments\n",
      "131039 likes\n"
     ]
    }
   ],
   "source": [
    "countCollections()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some boilerplate DB functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clearCollection(collection=None):\n",
    "    \n",
    "    all=False\n",
    "    answer=True\n",
    "    \n",
    "    if not collection or collection.lower().strip()=='all':\n",
    "        answer=raw_input('Clear all?')\n",
    "        if answer.lower().strip() in ['y','yes']:\n",
    "            all=True\n",
    "            \n",
    "    if all or collection=='likes':\n",
    "        if not all:answer=raw_input('Clear likes?')\n",
    "        if all or answer.lower().strip() in ['y','yes']:\n",
    "            res=likesCollection.remove()\n",
    "            print 'Cleared %d likes' % res['n']\n",
    "            \n",
    "    if all or collection=='pages':\n",
    "        if not all:answer=raw_input('Clear pages?')\n",
    "        if all or answer.lower().strip() in ['y','yes']:\n",
    "            res=pagesCollection.remove()\n",
    "            print 'Cleared %d pages' % res['n']\n",
    "\n",
    "            \n",
    "    if all or collection=='comments':\n",
    "        if not all:answer=raw_input('Clear comments?')\n",
    "        if all or answer.lower().strip() in ['y','yes']:\n",
    "            res=commentsCollection.remove()\n",
    "            print 'Cleared %d comments' % res['n']\n",
    "\n",
    "            \n",
    "    if all or collection=='posts':\n",
    "        if not all:answer=raw_input('Clear posts?')\n",
    "        if all or answer.lower().strip() in ['y','yes']:\n",
    "            res=postsCollection.remove()\n",
    "            print 'Cleared %d posts' % res['n']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1151 pages\n",
      "102108 posts\n",
      "208796 comments\n",
      "131039 likes\n"
     ]
    }
   ],
   "source": [
    "countCollections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isCommentInDb(id):\n",
    "    '''\n",
    "    Tests if a comment is in comments collection\n",
    "    Returns Bool\n",
    "    '''\n",
    "    nMatches=commentsCollection.find({'id':id}).count()\n",
    "    if nMatches==0:\n",
    "        return False\n",
    "    elif nMatches==1:\n",
    "        return True\n",
    "    else:\n",
    "#        logging.warning('Duplicate comment %s' % id)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addTimestampToPage(id):\n",
    "    '''\n",
    "    Adds current timestamp to page \n",
    "    '''\n",
    "    pagesCollection.update({'id':id},{'$set':{'checked':[int(time.time())]}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updatePageTimestamp(id):\n",
    "    '''\n",
    "    Updates timestamp of page \n",
    "    '''\n",
    "    pagesCollection.update({'id':id},{'$addToSet':{'checked':int(time.time())}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isPostInDb(id):\n",
    "    '''\n",
    "    Tests if a post is in post collection\n",
    "    Returns Bool\n",
    "    '''\n",
    "    nMatches=postsCollection.find({'id':id}).count()\n",
    "    if nMatches==0:\n",
    "        return False\n",
    "    elif nMatches==1:\n",
    "        return True\n",
    "    else:\n",
    "#        logging.warning('Duplicate post %s' % id)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isPageInDb(id):\n",
    "    '''\n",
    "    Tests if a page is in pages collection\n",
    "    Returns Bool\n",
    "    '''\n",
    "    nMatches=pagesCollection.find({'id':id}).count()\n",
    "    \n",
    "    if nMatches==0:\n",
    "        return False\n",
    "    elif nMatches==1:\n",
    "        return True\n",
    "    else:\n",
    "#        logging.warning('Duplicate page %s' % id)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to deal with API errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def handleResult(statusCode,returnText):\n",
    "    '''\n",
    "    Parses API call result to determine if successful\n",
    "    or to wait or abandon\n",
    "    Returns success,skip (both Bool)\n",
    "    '''\n",
    "    if statusCode==200:\n",
    "        # OK\n",
    "        return True,False\n",
    "    \n",
    "    if statusCode in [102,10,463,467]:\n",
    "        # Access token expired\n",
    "        logging.warning('API error: %d %s' % (statusCode,returnText))\n",
    "        return False,True\n",
    "    elif statusCode in [2,4,17,341,500]:\n",
    "        # Wait and retry\n",
    "        logging.warning('API error - waiting: %d %s' % (statusCode,returnText))\n",
    "        return False,False\n",
    "    elif statusCode in [506,1609005]:\n",
    "        # Skip\n",
    "        logging.warning('API error - skipping: %d %s' % (statusCode,returnText))\n",
    "        return False,True\n",
    "    else:\n",
    "        logging.warning('API error - unknown code %d %s' % (statusCode,returnText))\n",
    "        return False, True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start by looping through pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######################################################\n",
    "success=None\n",
    "nAttempts=0\n",
    "\n",
    "temp='https://graph.facebook.com/search?q=%s&limit=%d&type=page&access_token=%s' % (QUERY, LIMIT, ACCESSTOKEN)\n",
    "\n",
    "while not success:\n",
    "    # Keep looping if unsuccessful\n",
    "    r=requests.get(temp)\n",
    "    success,skip=handleResult(r.status_code,r.text)\n",
    "    # Try, find out if successful or should skip\n",
    "    \n",
    "    if skip or nAttempts==nSkip:\n",
    "        # If tried nSkip times or if should skip\n",
    "        r={'data':[],'paging':None}\n",
    "        if nAttempts==nSkip:\n",
    "            logging.warning('Skipping after %d attempts' % nAttempts)\n",
    "            break\n",
    "    time.sleep(nWait)\n",
    "    nAttempts+=1\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "getPostsFromPage(307091372652912)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getPages(data,paging,restart=None):\n",
    "    '''\n",
    "    Takes data returned from an API call searching for pages\n",
    "    '''\n",
    "    \n",
    "    if paging:\n",
    "        if paging.get('next'):\n",
    "            logging.warning('Paging needed')\n",
    "    \n",
    "    for n,d in enumerate(data):\n",
    "        \n",
    "        if (restart and d['id']==str(restart)) or (not restart):\n",
    "            # If restart id defined then wait until we find it\n",
    "            # if not add straight in\n",
    "            restart=None\n",
    "        \n",
    "            time.sleep(pageSleepTime)\n",
    "\n",
    "            print n,d['name']\n",
    "\n",
    "            if not langid.classify(d['name'])[0]=='en':\n",
    "                try:\n",
    "                    enName=textblob.TextBlob(d['name']).translate().string\n",
    "                    enName=clean(enName)\n",
    "                    print 'Translates: ',enName\n",
    "                except:\n",
    "                    logging.warning('Translation failed')\n",
    "                    enName=None\n",
    "            else:\n",
    "                enName=None\n",
    "\n",
    "            print 'http://fb.com/'+d['id'],d.get('category')\n",
    "            res=getPageInfo(d['id'],raw=True)\n",
    "\n",
    "            res['name']=clean(d['name'])\n",
    "            res['about']=clean(d.get('about'))\n",
    "            res['description']=clean(d.get('description'))\n",
    "\n",
    "            ######################################################################\n",
    "            posts,comments,likes=getPostsFromPage(d['id'],limit=postsLimit,raw=False)\n",
    "            # Get posts,comments,likes from that page\n",
    "            nAdded=nAlready=0\n",
    "            for post in posts:\n",
    "                if not isPostInDb(post['id']):\n",
    "                    addPostToDb(post)\n",
    "                    nAdded+=1\n",
    "                else:\n",
    "                    #logging.warning('Post %s already in DB' % post['id'])\n",
    "                    nAlready+=1\n",
    "            logging.warning('%d posts added (%d already in DB)' % (nAdded,nAlready))\n",
    "    #        addCommentsToDb(comments)\n",
    "\n",
    "    #        addLikesToDb(likes)\n",
    "            ######################################################################\n",
    "\n",
    "            if enName:\n",
    "                res['name_en']=enName\n",
    "\n",
    "            category=d.get('category')\n",
    "            if category:\n",
    "                res['category']=category\n",
    "\n",
    "            print res.keys()\n",
    "            if not isPageInDb(d['id']):\n",
    "                addPageToDb(res)\n",
    "            else:\n",
    "                logging.info('Page %s already in DB' % d['id'])\n",
    "            addTimestampToPage(d['id'])\n",
    "\n",
    "        else:\n",
    "            logging.warning('Skipping page %s. Waiting for %s to restart' % (d['id'],restart))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getPostsFromPage(pageId,raw=False,limit=100):\n",
    "    '''\n",
    "    Requests list of posts, list of comments and \n",
    "    list of likes from a page\n",
    "    Returns a list of JSON objects\n",
    "    or if raw=True, a string description of posts\n",
    "    '''\n",
    "    \n",
    "    logging.info('Getting posts,comments,likes for page %s' % pageId)\n",
    "    \n",
    "    tempUrl='https://graph.facebook.com/%s/posts?&limit=%d&access_token=%s' % (pageId,postsLimit,ACCESSTOKEN)\n",
    "    \n",
    "    out=[]\n",
    "    outFull=[]\n",
    "    \n",
    "    comments=None\n",
    "    likes=None\n",
    "    \n",
    "    r=requests.get(tempUrl)\n",
    "    ######################################################\n",
    "    success=None\n",
    "    nAttempts=0\n",
    "\n",
    "    while not success:\n",
    "        # Keep looping if unsuccessful\n",
    "        r=requests.get(tempUrl)\n",
    "        success,skip=handleResult(r.status_code,r.text)\n",
    "        # Try, find out if successful or should skip\n",
    "\n",
    "        if skip or nAttempts==nSkip:\n",
    "            # If tried nSkip times or if should skip\n",
    "            r={'data':[],'paging':None}\n",
    "            if nAttempts==nSkip:\n",
    "                logging.warning('Skipping posts after %d attempts' % nAttempts)\n",
    "                return ([],[],[])\n",
    "        time.sleep(nWait)\n",
    "        nAttempts+=1\n",
    "    ######################################################\n",
    "    \n",
    "    for n,d in enumerate(r.json()['data']):\n",
    "        \n",
    "        time.sleep(postSleepTime)\n",
    "\n",
    "        \n",
    "        name=d.get('name')\n",
    "        id=d.get('id')\n",
    "        print 'Post %d %s %s' % (n,name,id)\n",
    "        \n",
    "        message=d.get('message')\n",
    "        if message:\n",
    "            message=clean(message)\n",
    "        else:\n",
    "            logging.warning('No message for post %s' % d['id'])\n",
    "        \n",
    "        description=d.get('description')\n",
    "        if description:\n",
    "            description=clean(description)\n",
    "        else:\n",
    "            logging.warning('No description for post %s' % d['id'])\n",
    "        \n",
    "        caption=d.get('caption')\n",
    "        if caption:\n",
    "            caption=clean(caption)\n",
    "        else:\n",
    "            logging.warning('No caption for post %s' % d['id'])\n",
    "        \n",
    "        d['page_id']=pageId\n",
    "        d['retrieved']=time.time()\n",
    "        \n",
    "        if d.get('icon'):del d['icon']\n",
    "        if d.get('picture'):del d['picture']\n",
    "        if d.get('privacy'):del d['privacy']\n",
    "        # Don't need these\n",
    "        \n",
    "        try:\n",
    "            shareCount=d['shares']['count']\n",
    "            d['shares']=shareCount\n",
    "        except:\n",
    "            pass\n",
    "        # Simplify this\n",
    "        \n",
    "        if message:\n",
    "#            print message\n",
    "            out.append(message)\n",
    "            if not langid.classify(message)[0]=='en':\n",
    "                try:\n",
    "                    enMessage=textblob.TextBlob(message).translate().string\n",
    "                    enMessage=clean(enMessage)\n",
    "                    out.append('==>'+enMessage+'---------')\n",
    "                    d['en_message']=enMessage\n",
    "                except:\n",
    "                    logging.warning('Translation failed')\n",
    "                    enMessage=None\n",
    "        if description:\n",
    "            out.append(description)\n",
    "            if not langid.classify(description)[0]=='en':\n",
    "                try:\n",
    "                    enDescription=textblob.TextBlob(description).translate().string\n",
    "                    enDescription=clean(enDescription)\n",
    "                    out.append('==>'+enDescription+'---------')\n",
    "                    d['en_description']=enDescription\n",
    "                except:\n",
    "                    logging.warning('Translation failed')\n",
    "                    enDescription=None\n",
    "        if caption:\n",
    "            out.append(caption)\n",
    "            if not langid.classify(caption)[0]=='en':\n",
    "                try:\n",
    "                    enCaption=textblob.TextBlob(caption).translate().string\n",
    "                    enCaption=clean(enCaption)\n",
    "                    out.append('==>'+enCaption+'---------')\n",
    "                    d['en_caption']=enCaption\n",
    "                except:\n",
    "                    enCaption=None\n",
    "                    \n",
    "        try:\n",
    "            comments=d['comments']\n",
    "            del d['comments']\n",
    "        except:\n",
    "            comments=None\n",
    "        \n",
    "        if comments:\n",
    "            logging.info('Getting comments...')\n",
    "            commentData=getComments(comments,pageId)\n",
    "            # This does all the paging\n",
    "#            for c in commentData['data']:\n",
    "#                print 'Comments data:',c.keys()\n",
    "#                print 'Likes:',c[u'user_likes'],c.get('likes')\n",
    "            addCommentsToDb(commentData)\n",
    "            \n",
    "            # TODO get comment likes\n",
    "            \n",
    "        try:\n",
    "            likes=d['likes']\n",
    "            del d['likes']\n",
    "        except:\n",
    "            likes=None\n",
    "        \n",
    "        if likes:\n",
    "            logging.info('Getting likes...')\n",
    "            likeData=getLikes(likes,pageId,id)\n",
    "            # This does all the paging\n",
    "            addLikesToDb(likeData)\n",
    "\n",
    "        \n",
    "        outFull.append(d)       \n",
    "    \n",
    "    if raw:\n",
    "        '\\n'.join(out)\n",
    "        pass # return string\n",
    "    else:\n",
    "        return outFull,comments,likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getLikes(likes,pageId,id):\n",
    "    '''\n",
    "    Takes a dictionary of like data from API with keys\n",
    "    [paging,data], pageId and post id. If paging information is present, keep \n",
    "    requesting pages\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    if likes.get('paging'):\n",
    "        current=likes\n",
    "        \n",
    "        while current['paging'].get('next'):\n",
    "            logging.info('Paging likes... %s' % current['paging']['next'])\n",
    "            current=getNextLikes(current['paging']['next'])\n",
    "            if current:\n",
    "                print 'Got next page of likes (current) (%d so far)' % len(likes['data'])\n",
    "                likes['data'].extend(current['data'])\n",
    "            else:\n",
    "                break\n",
    "            # TODO better error handling\n",
    "                \n",
    "        print 'Got %d likes after paging' % len(likes['data'])\n",
    "#    print 'Likes type %s' % type(likes)\n",
    "#    print likes.keys()\n",
    "    \n",
    "    for like in likes['data']:\n",
    "        like['id']='%s_%s' % (id,like['id'])\n",
    "        # Make a unique like ID made up of post id_likeid\n",
    "        like['parent_id']=id\n",
    "        # Keep parent id of comment/post for getting most liked content\n",
    "    \n",
    "#    print 'Likes',likes.keys()\n",
    "#    print likes.get('paging')\n",
    "#    print likes['data'][0]\n",
    "    return likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getComments(comments,pageId):\n",
    "    '''\n",
    "    Takes a dictionary of comment data from API with keys\n",
    "    [paging,data]. If paging information is present, keep\n",
    "    requesting pages\n",
    "    '''\n",
    "    \n",
    "    if comments.get('paging'):\n",
    "        \n",
    "        current=comments\n",
    "        \n",
    "        while current['paging'].get('next'):\n",
    "            logging.info('Paging comments...')\n",
    "            current=getNextComments(current['paging']['next'])\n",
    "            comments['data'].extend(current['data'])\n",
    "            \n",
    "            if not current.get('paging'):\n",
    "                break\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getNextComments(nextToken):\n",
    "    '''\n",
    "    Given a paging token for next page of likes returns the raw data\n",
    "    '''\n",
    "    res=requests.get(nextToken)\n",
    "    \n",
    "    if not res.status_code==200:\n",
    "        logging.warning('Error with next comments data %d' % res.status_code)\n",
    "        return None\n",
    "    else:\n",
    "        return res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getNextLikes(nextToken):\n",
    "    '''\n",
    "    Given a paging token for next page of likes returns the raw data\n",
    "    '''\n",
    "#    logging.warning('Getting next likes: %s' % nextToken)\n",
    "    res=requests.get(nextToken)\n",
    "    \n",
    "    if not res.status_code==200:\n",
    "        logging.warning('Error with next likes data %d %s' % (res.status_code,res.text))\n",
    "        return None\n",
    "    else:\n",
    "        return res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getPageInfo(pageId,raw=False):\n",
    "    '''\n",
    "    Requests info for a page by ID\n",
    "    Returns the info either as a JSON object\n",
    "    or if raw=True as a string to be printed\n",
    "    '''\n",
    "    tempUrl='https://graph.facebook.com/v2.4/'+pageId+'?fields=about,description,location,phone,talking_about_count,\\\n",
    "    engagement,start_info,likes,website&access_token='+ACCESSTOKEN\n",
    "\n",
    "    res=requests.get(tempUrl)\n",
    "    \n",
    "    \n",
    "    if not res.status_code==200:\n",
    "        logging.warning('Request failed: %d %s' % (res.status_code,res.text))\n",
    "    \n",
    "    res=res.json()\n",
    "    res['retrieved']=time.time()\n",
    "    \n",
    "    description=res.get('description')\n",
    "    if description:\n",
    "        description=clean(description)\n",
    "    else:\n",
    "        logging.warning('No description for page %s' % pageId)\n",
    "        logging.warning('Keys %s' % res.keys())\n",
    "    \n",
    "    engagement=res.get('engagement')\n",
    "    if engagement:\n",
    "        if engagement.get(u'count'):\n",
    "            res['engagement']=str(res['engagement']['count'])\n",
    "    \n",
    "    start_info=res.get('start_info')\n",
    "    if start_info:\n",
    "        date=start_info.get(u'date')\n",
    "        if date:\n",
    "            res['start_info_clean']=str(res['start_info']['date']['year'])\n",
    "            if res['start_info']['date'].get('month'):\n",
    "                res['start_info_clean']+='/'+str(res['start_info']['date']['month'])\n",
    "                if res['start_info']['date'].get('day'):\n",
    "                    res['start_info_clean']+='/'+str(res['start_info']['date']['day'])\n",
    "\n",
    "        else:\n",
    "            del res['start_info']\n",
    "    \n",
    "    for k,v in res.items():\n",
    "        if type(v) in [unicode,str]:\n",
    "\n",
    "            if not langid.classify(v)[0]=='en':\n",
    "\n",
    "                try:\n",
    "                    res[k+'_en']=textblob.TextBlob(v).translate().string\n",
    "                except:\n",
    "                    logging.warning('Translation failed')\n",
    "                # Create a new dictionary entry with the translation\n",
    "    if raw:    \n",
    "        return res\n",
    "    else:\n",
    "#        print res.json().items()\n",
    "        return '\\n'.join([k+'\\t\\t'+unicode(v) for k,v in res.items()])+'\\n================'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "getPages(r.json()['data'],r.json().get('paging'),restart=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
